import os
import sys
import glob
import numpy as np
import pandas as pd
import math
import sys
import random
import pickle
import dask.dataframe as dd
from dask.distributed import Client, LocalCluster

import csv

sys.path.insert(0,'/home/djl34/lab_pd/bin')
import genomic

pd_data_dir = "/home/djl34/lab_pd/data"
KL_data_dir = "/home/djl34/lab_pd/kl/data"
aso_data_dir = "/home/djl34/lab_pd/aso/data"
scratch_dir = "/n/scratch/users/d/djl34"

vep = "/home/djl34/bin/ensembl-vep/vep"

factor = 1

def get_mem_mb(wildcards, attempt):
    return 20000 * factor + attempt * 10000

def get_mem_mb_AF(wildcards, attempt):
    return 30000 * factor + attempt * 10000

def get_mem_mb_small(wildcards, attempt):
    return attempt * 4000 * factor

wildcard_constraints:
    chrom="\d+"
    
per_generation_factor = 1.015 * 10 **-7

include: "/home/djl34/kl_git/preprocessing/download/Snakefile"

base_set = ["A", "C", "T", "G"]
all_chrom_set = [str(x) for x in range(1, 23)]
chrom_set = ["3"]

print(chrom_set)

##############################################################################################################################

type_list = ["ad_lof", "sd_lof", "ar_lof"]
type_list = ["ad_lof", "ar_lof"]


filename_list = [os.path.join(aso_data_dir, "genes_category/{type}/biomart/{chrom}.tsv"),
                 os.path.join(aso_data_dir, "whole_gene/mut_model/{type}/{chrom}.vcf"),
                 os.path.join(aso_data_dir, "whole_gene/vep_maxentscan/{type}/{chrom}_vep.vcf"),
                 os.path.join(aso_data_dir, "whole_gene/revel/{type}/{chrom}/_metadata")]

filename_list = [filename.format(type = type, chrom = chrom) for filename in filename_list for chrom in chrom_set for type in type_list]

print(chrom_set)

# os.path.join(aso_data_dir, "whole_gene/mut_model/{type}/{chrom}.vcf")

rule all:
    input:
        filename_list,
        # os.path.join(aso_data_dir, "genes_category/{type}/ad_rate/pergene_all.tsv").format(type = "ad_lof"),
        # os.path.join(aso_data_dir, "genes_category/{type}/ad_rate/pergene_all.tsv").format(type = "sd_lof"),
        # os.path.join(aso_data_dir, "genes_category/{type}/ar_rate/pergene_all.tsv").format(type = "ar_lof"),
        # os.path.join(aso_data_dir, "genes_category/{type}/ar_rate/pergene_all.tsv").format(type = "sd_lof"),
        # os.path.join(aso_data_dir, "whole_gene/ENSG00000163930/_metadata"),


###################################################### get gnomAD #######################################################
        
rule run_vep_maxentscan:
    input:
        os.path.join(scratch_dir, "downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{chrom}.vcf.bgz")
    output:
        os.path.join(scratch_dir, "downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{chrom}.MaxEntScan.txt")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    shell:
        """
        module load gcc/6.2.0
        module load perl/5.30.0
        eval `perl -Mlocal::lib=~/perl5-O2`
        {vep} --cache -i /home/djl34/scratch/downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{wildcards.chrom}.vcf.bgz -o /home/djl34/scratch/downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{wildcards.chrom}.MaxEntScan.txt --fork 5 --plugin MaxEntScan,$HOME/.vep/Plugins/fordownload --force_overwrite --no_stats
        """
         
# rule gnomAD_v3_to_tsv:
#     input:
#         os.path.join(scratch_dir, "downloads/gnomad.genomes.v3.1.2.sites.chr{chrom}.vcf.bgz")
#     output:
#         os.path.join(scratch_dir, "downloads/gnomad.genomes.v3.1.2.sites.chr{chrom}.tsv")
#     shell:        
#         "bcftools query -f '%CHROM\t%POS\t%REF\t%ALT\t%FILTER\t%AC\t%AN\t%AF\n' {input} > {output}"
        
###################################################### set up Ensembl #######################################################

rule download_ensembl_cache:
    input:
    output:
        "/home/djl34/.vep/homo_sapiens_vep_110_GRCh38.tar.gz"
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    shell:
        """
        open_perl5
        cd $HOME/.vep
        curl -O https://ftp.ensembl.org/pub/release-110/variation/indexed_vep_cache/homo_sapiens_vep_110_GRCh38.tar.gz
        """
        
        
###################################################### for MaxEntScan #######################################################

rule make_regions_file:
    input:
        gene = os.path.join(aso_data_dir, "genes_category/{type}.tsv"),
        biomart = os.path.join(pd_data_dir, "biomart/ENSG_start_end_110.tsv"),
    output:
        os.path.join(aso_data_dir, "genes_category/{type}/biomart/{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=2000
    run:        
        df = pd.read_csv(input.biomart, sep = "\t")
        df = df[df["Chromosome/scaffold name"] == wildcards.chrom]
        df = df.sort_values("Gene start (bp)")
        df = df.rename({"Gene stable ID": "Gene"}, axis = 1)

        df_type = pd.read_csv(input.gene, sep = "\t")

        df = df[df["Gene"].isin(list(df_type["Gene"].unique()))]

        df = df[["Chromosome/scaffold name", "Gene start (bp)", "Gene end (bp)"]]
        df.to_csv(output[0], sep = "\t", index = None, header = None)

                             
        # iterator = 0
        
        # csvfile = open(os.path.join(scratch_dir, "downloads/whole_gene/" + str(wildcards.chrom) + "_regions_split_"+ str(iterator)+".tsv"), 'w', newline='')
        # csvwriter = csv.writer(csvfile, delimiter='\t')
        
        # total_length = 0
        
        # for index, row in df.iterrows():
            
        #     chrom = row["Chromosome/scaffold name"]
        #     start = row["Gene start (bp)"]
        #     end = row["Gene end (bp)"]
            
        #     length = end - start
            
        #     while total_length + length > 1000000:
        #         add = 1000000 - total_length
                
        #         new_end = start + add
                
        #         csvwriter.writerow([chrom, start, new_end])
                
        #         #move to new file
        #         iterator += 1
        #         csvfile = open(os.path.join(scratch_dir, "downloads/whole_gene/" + str(wildcards.chrom) + "_regions_split_" + str(iterator)+ ".tsv"), 'w', newline='')
        #         csvwriter = csv.writer(csvfile, delimiter='\t')
        #         total_length = 0
                
        #         start = new_end + 1
        #         length = end - start
                
        #     csvwriter.writerow([chrom, start, end])
            
        #     total_length += length
            
#             if total_length > 1000000:
#                 total_length = 0
#                 iterator += 1
                
#                 csvfile = open(os.path.join(scratch_dir, "downloads/whole_gene/" + str(wildcards.chrom) + "_regions_split_" + str(iterator)+ ".tsv"), 'w', newline='')
#                 csvwriter = csv.writer(csvfile, delimiter='\t')
                        

# rule download_vova_model:
#     input:
# #         os.path.join(pd_data_dir, "vova_model/{chrom}_rate_v5.2_TFBS_correction.gz")
#     output:
#         os.path.join(scratch_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz"),
#         os.path.join(scratch_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz.csi"),
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=5,
#         mem_mb=25000
#     run:
#         output_filename_0 = output[0].split("/")[-1]
#         output_filename_1 = output[1].split("/")[-1]
        
#         shell("wget -P /home/djl34/scratch/downloads/ http://genetics.bwh.harvard.edu/downloads/Vova/Roulette/{output_filename_0}")
#         shell("wget -P /home/djl34/scratch/downloads/ http://genetics.bwh.harvard.edu/downloads/Vova/Roulette/{output_filename_1}")
        
# rule filter_out_intergenic_sites:
#     input:
#         vcf = os.path.join(scratch_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz"),
#         regions_file = os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_{split}.tsv")
#     output:
#         os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_{split}_rate_v5.2_TFBS_correction_wholegene.vcf"),
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=1,
#         mem_mb=3000
#     shell:        
#         "bcftools view -R {input.regions_file} -o {output} {input.vcf}"


rule convert_to_vcf_file:
    input:
        region = os.path.join(aso_data_dir, "genes_category/{type}/biomart/{chrom}.tsv"),
        mut_model = os.path.join(KL_data_dir, "whole_genome/mu_filtered/{chrom}/_metadata")
    output:
        os.path.join(aso_data_dir, "whole_gene/mut_model/{type}/{chrom}.vcf")
    resources:
        partition="short",
        runtime="0-0:30",
        cpus_per_task=1,
        mem_mb=5000
    run:
        with Client() as client:

            header = ["Chrom", "start", "end"]
            df = pd.read_csv(input.region, sep = "\t", names = header)

            if len(df) == 0:
                df.to_csv(output[0], sep = "\t", index = None, header = None)
            else:
                def get_range(start, end):
                    # since this seems like it's one-based
                    return range(start, end + 1)
    
                df["Pos"] = df.apply(lambda row: get_range(row["start"],row["end"]), axis=1)
                df = df.explode('Pos')
                df["Pos"] = df["Pos"].astype(int)
    
                filename = input.mut_model
                rate = dd.read_parquet(filename.split("_metadata")[0])
                rate = rate[rate["Pos"].isin(list(df["Pos"]))]
    
                rate["Chrom"] = int(wildcards.chrom)
                rate["Pos2"] = rate["Pos"]
                rate["mut_type"] = rate["Allele_ref"] + "/" + rate["Allele"]
                rate["strand"] = "+"
                
                rate[["Chrom", "Pos", "Pos2", "mut_type", "strand"]].to_csv(output[0], sep = "\t", index = None, header = None, single_file = True)
            
        
rule run_vep_maxentscan_whole_gene:
    input:
        os.path.join(aso_data_dir, "whole_gene/mut_model/{type}/{chrom}.vcf"),
    output:
        os.path.join(aso_data_dir, "whole_gene/vep_maxentscan/{type}/{chrom}_vep.vcf"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=8,
        mem_mb=32000
    shell:
        """
        module load gcc/6.2.0
        module load perl/5.30.0
        eval `perl -Mlocal::lib=~/perl5-O2`
        {vep} --cache --offline -i {input} -o {output} --fork 8 --vcf --canonical --plugin MaxEntScan,$HOME/.vep/Plugins/fordownload --force_overwrite --no_stats --buffer_size 10000 --fasta /home/djl34/lab_pd/data/fasta/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna
        """
        
rule run_split_vep:
    input:
        os.path.join(aso_data_dir, "whole_gene/vep_maxentscan/{type}/{chrom}_vep.vcf"),
    output:
        os.path.join(scratch_dir, "aso/whole_gene/split_vep/{type}/{chrom}_primary_vep.txt"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=2000
    shell:
#-s primary picks all of the canonical variants
        """
        bcftools +split-vep {input} -f '%CHROM\t%POS\t%REF\t%ALT\t%MR\t%Consequence\t%Gene\t%Feature\t%MaxEntScan_alt\t%MaxEntScan_diff\t%MaxEntScan_ref\n' -s primary > {output}
        """
        
rule make_parquet:
    input:
        os.path.join(scratch_dir, "aso/whole_gene/split_vep/{type}/{chrom}_primary_vep.txt"),
    output:
        os.path.join(aso_data_dir, "whole_gene/vep_primary/{type}/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=6000
    run:
        with Client() as client:
            names_list = ['CHROM', 'POS', 'REF', 'ALT', 'Consequence', 'Gene', 'Feature', 'MaxEntScan_alt', 'MaxEntScan_diff', 'MaxEntScan_ref']
            ddf = dd.read_csv(os.path.join(scratch_dir, "downloads/whole_gene/"+ wildcards.chrom + "_regions_split_*_rate_v5.2_TFBS_correction_wholegene.MaxEntScan.txt"), sep = "\t", names = names_list, dtype={'MaxEntScan_alt': 'object','MaxEntScan_diff': 'object', 'MaxEntScan_ref': 'object'})
            
            ddf= ddf.rename(columns={"POS": "Pos", "REF": "Allele_ref", "ALT": "Allele"})

            ddf["Pos"] = ddf["Pos"].astype('Int64')
            
            ddf = ddf.repartition(partition_size="3GB")            
            
            ddf.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

###################################################### for mu #########################################################
            
rule add_mu:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/vep_primary/{type}/{chrom}/_metadata"),
        mu = os.path.join(KL_data_dir, "whole_genome/mu_filtered/{chrom}/_metadata")
    output:
        os.path.join(aso_data_dir, "whole_gene/mu/{type}/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_parquet(input.rate.split("_metadata")[0])

            mu = dd.read_parquet(input.mu.split("_metadata")[0])

            mu = mu[["Pos", "Allele", "mu"]]

            rate = rate.merge(mu, on = ["Pos", "Allele"], how = "left")

            rate = rate.repartition(partition_size="3GB")            

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)   
        
###################################################### for spliceAI #######################################################

# rule process_spliceAI_vcf:    
#     input:
#         vcf = os.path.join(scratch_dir, "downloads/spliceai/spliceai_scores.raw.snv.chrom_{chrom}.hg38.vcf"),
#     output:
#         os.path.join(aso_data_dir, "spliceai/spliceai_delta_scores.raw.snv.chrom_{chrom}.tsv")
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         with Client() as client:
#             names = ["CHROM", "Pos", "ID","Allele_ref", "Allele", "QUAL", "FILTER", "Spliceai_info"]
#             ddf = dd.read_csv(input.vcf, sep = "\t", comment = "#", names = names, dtype={'CHROM': 'int', 'Pos': 'int'})
            
#             ddf["Pos"] = dd.to_numeric(ddf['Pos'], errors='coerce').fillna(0).astype(int)
#             ddf = ddf[ddf["Pos"].isna() == False]
#             ddf = ddf[ddf["Allele_ref"].isna() == False]
#             ddf = ddf[ddf["Allele"].isna() == False]
            
#             ddf[["Pos", "Allele_ref", "Allele", "Spliceai_info"]].to_csv(output[0], sep = "\t", index = None, single_file = True)


rule add_spliceAI_vcf:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/mu/{type}/{chrom}/_metadata"),
        spliceai = os.path.join(scratch_dir, "downloads/spliceai/spliceai_scores.raw.snv.chrom_{chrom}.hg38.vcf")
    output:
        os.path.join(aso_data_dir, "whole_gene/spliceai_vcf/{type}/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet(input.rate.split("_metadata")[0])
            
            names = ["Chrom", "Pos", "ID", "Allele_ref", "Allele", "QUAL", "FILTER", "Spliceai_info"]
            ddf = dd.read_csv(input.spliceai, sep = "\t", names = names, comment = "#", dtype={'Pos': 'Int64'})
            
            rate = rate.merge(ddf[["Pos", "Allele_ref", "Allele", "Spliceai_info"]], on = ["Pos", "Allele_ref", "Allele"], how = "left")                        
            rate = rate.repartition(partition_size="3GB")            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

###################################################### for LaBranchoR #######################################################

rule add_LaBranchoR:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/spliceai_vcf/{type}/{chrom}/_metadata"),
        labranchor = aso_data_dir + "/LaBranchoR/lstm.gencode_v19.hg19.top.bed",
    output:
        os.path.join(aso_data_dir, "whole_gene/LaBranchoR/{type}/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-5:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            ddf = dd.read_parquet(input.rate.split("_metadata")[0])
            
            names_list = ["Chrom", "start", "end", "LaBranchoR_start", "LaBranchoR_score", "strand"]
            df = pd.read_csv(aso_data_dir + "/LaBranchoR/lstm.gencode_v19.hg19.top.bed", sep = "\t", names = names_list)
            df = df[df["Chrom"] == "chr" + wildcards.chrom]
            
            df["hg38_start"] = df.apply(lambda row: genomic.get_hg38_pos(row["Chrom"], row["start"]), axis=1)
            df["hg38_end"] = df.apply(lambda row: genomic.get_hg38_pos(row["Chrom"], row["end"]), axis=1)
            
            #b/c it is 0 based
            def get_range(start, end):
                return range(start + 1, end + 1)
            
            df = df[df["hg38_start"].isna() == False]
            df = df[df["hg38_end"].isna() == False]
            
            df["hg38_start"] = df["hg38_start"].astype(int)
            df["hg38_end"] = df["hg38_end"].astype(int)
            
            df['hg38_min'] = df[['hg38_start','hg38_end']].min(axis=1)
            df['hg38_max'] = df[['hg38_start','hg38_end']].max(axis=1)

            
            df["Pos"] = df.apply(lambda row: get_range(row["hg38_min"],row["hg38_max"]), axis=1)
            df = df.explode('Pos')
            df["Pos"] = df["Pos"].astype("Int64")
            
            df["LaBranchoR_distance"] = 0
            
            for distance in [-3, -2, -1, 1, 2, 3]:
                df_nearby = df.copy()
                
                df_nearby["LaBranchoR_distance"] = distance
                df_nearby["Pos"] = df_nearby["Pos"] + distance
                
                df = pd.concat([df, df_nearby])
                
            
            ddf = ddf.merge(df[["Pos", "LaBranchoR_score", "LaBranchoR_distance"]], on = "Pos", how = "left")
            
            ddf.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True) 
            
        
###################################################### for REVEL #######################################################

# rule dowload_revel:
#     input:
#     output:
#         os.path.join(scratch_dir, "downloads/revel/revel-v1.3_all_chromosomes.zip")
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=1,
#         mem_mb=get_mem_mb_small
#     shell:
#         "wget -P /n/scratch3/users/d/djl34/downloads/revel/ https://www.google.com/url?q=https%3A%2F%2Frothsj06.dmz.hpc.mssm.edu%2Frevel-v1.3_all_chromosomes.zip&sa=D&sntz=1&usg=AOvVaw2DS2TWUYl__0vqijzzxp5M"

rule add_revel:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/LaBranchoR/{type}/{chrom}/_metadata"),
        revel = aso_data_dir + "/revel/revel_with_transcript_ids",
    output:
        os.path.join(aso_data_dir, "whole_gene/revel/{type}/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-3:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            ddf = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            revel = dd.read_csv(input.revel, dtype={'chr': 'object', 'grch38_pos': 'object'})
            
            revel = revel[revel["chr"] == wildcards.chrom]
            
            revel = revel.rename(columns = {"grch38_pos": "Pos", "ref" : "Allele_ref", "alt": "Allele", "Ensembl_transcriptid": "REVEL_transcriptid"})
            
            revel = revel.compute()
            revel = revel[revel["Pos"] != "."]
            revel["Pos"] = revel["Pos"].astype("Int64")
            
            ddf = ddf.merge(revel[["Pos", "Allele", "REVEL", "REVEL_transcriptid"]], on = ["Pos", "Allele"], how = "left")
            
            ddf = ddf.repartition(partition_size="3GB")            

            ddf.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
rule work_on_clinvar_data:
    input:
        vcf = os.path.join(aso_data_dir, "clinvar/clinvar.vcf.gz"),
    output:
        os.path.join(aso_data_dir, "clinvar/clinvar_{chrom}.vcf"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=3000
    shell:        
        "bcftools view -r {wildcards.chrom} -o {output} {input.vcf}"
            
rule clinvar_to_tsv:
    input:
        os.path.join(aso_data_dir, "clinvar/clinvar_{chrom}.vcf"),
    output:
        os.path.join(aso_data_dir, "clinvar/clinvar_{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=3000
    shell:        
        "bcftools query -f '%CHROM\t%POS\t%REF\t%ALT\t%CLNDN\t%CLNDISDB\t%CLNSIG\t%CLNSIGCONF\t%GENEINFO\n' {input} > {output}"
        
            
# rule add_revel:
#     input:
#         rate = os.path.join(aso_data_dir, "whole_gene/LaBranchoR/{chrom}/_metadata"),
#         revel = aso_data_dir + "/revel/revel_with_transcript_ids",
#     output:
#         os.path.join(aso_data_dir, "whole_gene/revel/{chrom}/_metadata"),
#     resources:
#         partition="short",
#         runtime="0-3:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:
#         with Client() as client:
            
#             ddf = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
#             revel = dd.read_csv(input.revel, dtype={'chr': 'object', 'grch38_pos': 'object'})
            
#             revel = revel[revel["chr"] == wildcards.chrom]
            
#             revel = revel.rename(columns = {"grch38_pos": "Pos", "ref" : "Allele_ref", "alt": "Allele", "Ensembl_transcriptid": "REVEL_transcriptid"})
            
#             revel = revel.compute()
#             revel = revel[revel["Pos"] != "."]
#             revel["Pos"] = revel["Pos"].astype(int)
            
#             ddf = ddf.merge(revel[["Pos", "Allele", "REVEL", "REVEL_transcriptid"]], on = ["Pos", "Allele"], how = "left")
            
#             ddf = ddf.repartition(partition_size="3GB")            

#             ddf.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            
            
#             ddf = ddf.drop_duplicates(["Pos", "Allele", "Gene"])

# rule download_gnomad_v4:
#     input:
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=1,
#         mem_mb=500
#     output:
#         os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.sites.chr{chrom}.vcf.bgz")
#     shell:
#         "wget -P /n/scratch/users/d/djl34/downloads/ https://storage.googleapis.com/gcp-public-data--gnomad/release/4.0/vcf/exomes/gnomad.exomes.v4.0.sites.chr{wildcards.chrom}.vcf.bgz"
                
# rule download_gnomad_v4_tbi:
#     input:
#     resources:
#         partition="short",
#         runtime="0-1:00",
#         cpus_per_task=1,
#         mem_mb=500
#     output:
#         os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.sites.chr{chrom}.vcf.bgz.tbi")
#     shell:
#         "wget -P /n/scratch/users/d/djl34/downloads/ https://storage.googleapis.com/gcp-public-data--gnomad/release/4.0/vcf/exomes/gnomad.exomes.v4.0.sites.chr{wildcards.chrom}.vcf.bgz.tbi"
                
        
            
rule run_split_vep_gnomAD_v4:
    input:
        vcf = os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.sites.chr{chrom}.vcf.bgz"),
        tbi = os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.sites.chr{chrom}.vcf.bgz.tbi")
    output:
        os.path.join(scratch_dir, "downloads/gnomad.exomes.v4.0.sites.chr{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=1,
        mem_mb=1000
    shell:
        """
        bcftools query -f '%CHROM\t%POS\t%REF\t%ALT\t%FILTER\t%AC\t%AN\t%AF\n' {input.vcf}  > {output}
        """
                    
##########################################################################################################################
        
######################################################filter genes #######################################################

################# let's first just get a list of genes

#for AD genes
rule AD_lof_genes:
    input:
        disease = aso_data_dir + "/clingen/Clingen-Gene-Disease-Summary-2023-06-24.csv",
        dosage = aso_data_dir + "/clingen/Clingen-Dosage-Sensitivity-2023-06-25.csv",
        ensg = pd_data_dir + "/biomart/ENSG_Genename_syn.tsv"
    output:
        os.path.join(aso_data_dir, "genes_category/ad_lof.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        #get list of genes that are ad and lof
        df = pd.read_csv(input.disease)

        df_ds = pd.read_csv(input.dosage)
        df_ds_haploinsufficient = df_ds[df_ds["HAPLOINSUFFICIENCY"].isin(['Sufficient Evidence for Haploinsufficiency', 'Emerging Evidence for Haploinsufficiency'])]

        df_AD = df[df["MOI"] == "AD"]

        df_AD_strong = df_AD[df_AD["CLASSIFICATION"].isin(["Strong", "Definitive"])]

        df_AD_strong_lof = df_AD_strong.merge(df_ds_haploinsufficient, on = "GENE SYMBOL", how = "inner")

        gene_list = list(df_AD_strong_lof["GENE SYMBOL"].unique())

        #add ensg name
        df_ensg = pd.read_csv(input.ensg, sep = "\t")
        df_ensg = df_ensg.rename({"Gene name": "GENE SYMBOL", "Gene stable ID" : "Gene"}, axis = 1)
        df_ensg = df_ensg[["GENE SYMBOL", "Gene", "Chromosome/scaffold name"]].drop_duplicates()
        df_ensg = df_ensg[df_ensg["Chromosome/scaffold name"].str.contains("CHR") == False]
        df_ensg = df_ensg.rename({"Gene name": "GENE SYMBOL", "Gene stable ID" : "Gene"}, axis = 1)

        df_AD_strong_lof_ensg = df_AD_strong_lof.merge(df_ensg, on = "GENE SYMBOL", how = "left")

        ensg_gene_list = list(df_AD_strong_lof_ensg["Gene"].unique())
        
        df_AD_strong_lof_ensg.to_csv(output[0], sep = "\t", index = None)

#for SD genes
rule SD_lof_genes:
    input:
        disease = aso_data_dir + "/clingen/Clingen-Gene-Disease-Summary-2023-06-24.csv",
        ensg = pd_data_dir + "/biomart/ENSG_Genename_syn.tsv"
    output:
        os.path.join(aso_data_dir, "genes_category/sd_lof.tsv"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            #get list of genes that are ad and lof
            df = pd.read_csv(input.disease)
            
            df_AR = df[df["MOI"] == "SD"]
            df_AR_strong = df_AR[df_AR["CLASSIFICATION"].isin(["Strong", "Definitive"])]
            
            gene_list = list(df_AR_strong["GENE SYMBOL"].unique())
            
            #add ensg name
            df_ensg = pd.read_csv(input.ensg, sep = "\t")
            df_ensg = df_ensg.rename({"Gene name": "GENE SYMBOL", "Gene stable ID" : "Gene"}, axis = 1)
            df_ensg = df_ensg[["GENE SYMBOL", "Gene", "Chromosome/scaffold name"]].drop_duplicates()
            df_ensg = df_ensg[df_ensg["Chromosome/scaffold name"].str.contains("CHR") == False]
            df_ensg = df_ensg.rename({"Gene name": "GENE SYMBOL", "Gene stable ID" : "Gene"}, axis = 1)

            df_AR_strong = df_AR_strong.merge(df_ensg, on = "GENE SYMBOL", how = "left")

            ensg_gene_list = list(df_AR_strong["Gene"].unique())
            
            df_AR_strong.to_csv(output[0], sep = "\t", index = None)
        
        
#for AR genes
rule AR_lof_genes:
    input:
        disease = aso_data_dir + "/clingen/Clingen-Gene-Disease-Summary-2023-06-24.csv",
        ensg = pd_data_dir + "/biomart/ENSG_Genename_syn.tsv"
    output:
        os.path.join(aso_data_dir, "genes_category/ar_lof.tsv"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            #get list of genes that are ad and lof
            df = pd.read_csv(input.disease)
            
            df_AR = df[df["MOI"] == "AR"]
            df_AR_strong = df_AR[df_AR["CLASSIFICATION"].isin(["Strong", "Definitive"])]
            
            gene_list = list(df_AR_strong["GENE SYMBOL"].unique())
            
            #add ensg name
            df_ensg = pd.read_csv(input.ensg, sep = "\t")
            df_ensg = df_ensg.rename({"Gene name": "GENE SYMBOL", "Gene stable ID" : "Gene"}, axis = 1)
            df_ensg = df_ensg[["GENE SYMBOL", "Gene", "Chromosome/scaffold name"]].drop_duplicates()
            df_ensg = df_ensg[df_ensg["Chromosome/scaffold name"].str.contains("CHR") == False]
            df_ensg = df_ensg.rename({"Gene name": "GENE SYMBOL", "Gene stable ID" : "Gene"}, axis = 1)

            df_AR_strong = df_AR_strong.merge(df_ensg, on = "GENE SYMBOL", how = "left")

            ensg_gene_list = list(df_AR_strong["Gene"].unique())
            
            df_AR_strong.to_csv(output[0], sep = "\t", index = None)

# ## for genes in top ventile
# rule AD_lof_top_ventile:
#     input:
#         disease = aso_data_dir + "/clingen/Clingen-Gene-Disease-Summary-2023-06-24.csv",
#         dosage = aso_data_dir + "/clingen/Clingen-Dosage-Sensitivity-2023-06-25.csv",
#         ensg = pd_data_dir + "/biomart/ENSG_Genename_syn.tsv"
#     output:
#         os.path.join(aso_data_dir, "genes_category/ad_lof.tsv"),
#     resources:
#         partition="short",
#         runtime="0-12:00",
#         cpus_per_task=5,
#         mem_mb=get_mem_mb
#     run:

############### from the genes, get the variants

rule get_gene_variants:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/revel/3/_metadata"),
    output:
        os.path.join(aso_data_dir, "whole_gene/ENSG00000163930/_metadata"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            #read variants file
            variants = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")

            variants = variants[variants["Gene"] == 'ENSG00000163930']
                        
            variants.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
        


rule gene_to_variants:
    input:
        gene = os.path.join(aso_data_dir, "genes_category/{type}.tsv"),
        rate = os.path.join(aso_data_dir, "whole_gene/revel/{chrom}/_metadata"),
    output:
        os.path.join(aso_data_dir, "genes_category/{type}/variants/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-2:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            #get list of genes that are ad and lof
            df = pd.read_csv(input.gene, sep = "\t")
            
            #read variants file
            variants = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            ensg_gene_list = list(df["Gene"].unique())
            variants = variants[variants["Gene"].isin(ensg_gene_list)]
            
#             variants["MaxEntScan_alt"] = variants["MaxEntScan_alt"].replace(".", 0.0)
#             variants["MaxEntScan_ref"] = variants["MaxEntScan_ref"].replace(".", 0.0)
            
#             variants["MaxEntScan_alt"] = dd.to_numeric(variants['MaxEntScan_alt'], errors='coerce').fillna(0)
#             variants["MaxEntScan_ref"] = dd.to_numeric(variants['MaxEntScan_ref'], errors='coerce').fillna(0)
            
            #split spliceAI_info
            columns = ["ALLELE", "SYMBOL", "DS_AG", "DS_AL", "DS_DG", "DS_DL", "DP_AG", "DP_AL", "DP_DG", "DP_DL"]

            variants[columns] = variants["Spliceai_info"].str.split("|", expand = True, n = 9)
            
            variants = variants.drop(["ALLELE", "Spliceai_info"], axis = 1)
            
            variants["DS_AG"] = variants["DS_AG"].astype(float)
            variants["DS_AL"] = variants["DS_AL"].astype(float)
            variants["DS_DG"] = variants["DS_DG"].astype(float)
            variants["DS_DL"] = variants["DS_DL"].astype(float)
            
            variants = variants.repartition(partition_size="3GB")
            
#             variants = variants.drop_duplicates(["Pos", "Allele", "Gene"])
                        
            variants.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
        
        
rule process_file:
    input:
        rate = os.path.join(aso_data_dir, "genes_category/{type}/variants/{chrom}/_metadata"),
    output:
        os.path.join(aso_data_dir, "genes_category/{type}/process/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        import pyarrow as pa
        
        with Client() as client:
            
            #read variants file
            variants = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            variants["MaxEntScan_alt"] = variants["MaxEntScan_alt"].replace(".", None)
            variants["MaxEntScan_ref"] = variants["MaxEntScan_ref"].replace(".", None)

            variants["MaxEntScan_alt"] = dd.to_numeric(variants['MaxEntScan_alt'], errors='coerce').fillna(0.0)
            variants["MaxEntScan_ref"] = dd.to_numeric(variants['MaxEntScan_ref'], errors='coerce').fillna(0.0)
            
            variants = variants.drop(columns=['REVEL_transcriptid', 'SYMBOL', 'DP_AG', 'DP_AL', 'DP_DG', 'DP_DL'])
                                    
            variants.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", 
                                write_metadata_file = True, 
                                schema={"MaxEntScan_alt": pa.float64(), "MaxEntScan_ref": pa.float64()})

rule add_AF:
    input:
        rate = os.path.join(aso_data_dir, "genes_category/{type}/process/{chrom}/_metadata"),
        add = os.path.join(KL_data_dir, "whole_genome/filtered/{chrom}/_metadata")
    output:
        os.path.join(aso_data_dir, "genes_category/{type}/AF/{chrom}/_metadata")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb_AF
    run:
        with Client() as client:
            variants = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            ddf = dd.read_parquet("/".join(input[1].split("/")[:-1]) + "/")

            variants["Pos"] = variants["Pos"].astype('Int64')

            variants = variants.merge(ddf[["Pos", "Allele", 'AF']], on = ["Pos", "Allele"], how = "left")

            variants.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

rule add_clinVar:
    input:
        rate = os.path.join(aso_data_dir, "genes_category/{type}/AF/{chrom}/_metadata"),
        clinvar = aso_data_dir + "/clinvar/clinvar_{chrom}.tsv"
    output:
        os.path.join(aso_data_dir, "aso/genes_category/{type}/clinvar/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb_AF
    run:
        with Client() as client:
            variants = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            variants = variants[~variants["mu"].isna()]

            names_list = ['Chrom', 'Pos', 'Allele_ref', 'Allele', 'CLNDN', 'CLNDISDB', 'CLNSIG', 'CLNSIGCONF', 'GENEINFO']
            clinvar = pd.read_csv(input.clinvar, sep = "\t", names = names_list)
            clinvar["Pos"] = clinvar["Pos"].astype('Int64')
            
            clinvar = clinvar.drop("Chrom", axis = 1)
            variants = variants.merge(clinvar, on = ["Pos", "Allele_ref", "Allele"], how = "left")

            variants = variants.repartition(partition_size="3GB")  

            variants.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)


            
############################################### calculate splicing probability ###############################################       
rule add_splicing_info:
    input:
        os.path.join(aso_data_dir, "aso/genes_category/{type}/clinvar/{chrom}/_metadata")
    output:
        os.path.join(aso_data_dir, "genes_category/{type}/splice/{chrom}/_metadata")
    resources:
        partition="short",
        runtime="0-01:30",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            variants = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            variants["canonical_splice_site"] = variants["Consequence"].str.contains("splice_(acceptor|donor)_variant")

            variants["spliceAI_loss_pass"] = (variants["DS_AL"] >= 0.1) | (variants["DS_DL"] >= 0.1)
            variants["spliceAI_gain_pass"] = (variants["DS_AG"] >= 0.1) | (variants["DS_DG"] >= 0.1)
            
            variants["MaxEntScan_alt_low"] = (variants["MaxEntScan_alt"] < 2) | (variants["MaxEntScan_alt"] < 0.3 * variants["MaxEntScan_ref"])

            ## check whether it is a splice-destroying-variant
            ##  SpliceAI donor/acceptor loss score ≥ 0.1 at a canonical splice site) AND (MaxEntScan donor/ acceptor score with the ALT allele < MaxEntScan donor/acceptor score with the REF allele) AND [(MaxEntScan donor/acceptor score with the ALT allele < 2) OR (MaxEntScan donor/acceptor score with the ALT allele < 0.3 × MaxEntScan donor/acceptor score with the REF allele)].
            
            variants["splice_destroying_severe"] = (variants["canonical_splice_site"] & variants["spliceAI_loss_pass"] & 
                                                    (variants["MaxEntScan_alt"] < variants["MaxEntScan_ref"]) &
                                                    variants["MaxEntScan_alt_low"])
                                                    

            ## check whether it is a splice-destroying-variant moderate
            ## i) NOT severe (as defined above), (ii) SpliceAI donor/ acceptor loss score at a canonical splice site ≥ 0.1, AND (iii) [MaxEntScan donor/acceptor score with the ALT allele at the site < MaxEntScan donor/acceptor score with the REF allele at the site, MaxEntScan donor/ acceptor score with the ALT allele at the site ≥ 2, AND MaxEntScan donor/acceptor score with the ALT allele at the site ≥ 0.3 × MaxEntScan donor/acceptor score with the REF allele at the site] OR [The variant is ≤3 nt away from the LaBranchoR-predicted branchpoint OR the distance between the LaBranchoR-predicted branchpoint and the site is changed by >3 nt by the variant].

            variants["LaBranchoR_distance"] = variants["LaBranchoR_distance"].fillna(0)
            variants["LaBranchoR_close"] = (variants["LaBranchoR_distance"] <= 3) & (variants["LaBranchoR_distance"] >= -3)

            variants["splice_destroying_moderate"] = (~variants["splice_destroying_severe"] & 
                                                      variants["canonical_splice_site"] & variants["spliceAI_loss_pass"] &
                                                      (((variants["MaxEntScan_alt"] < variants["MaxEntScan_ref"]) &
                                                      ~variants["MaxEntScan_alt_low"]) | variants["LaBranchoR_close"]))
            
            ##check whether misplicing gain
            ##spliceAI donor/ acceptor gain score at a non-canonical site ≥ 0.1 AND (ii) MaxEntScan donor/acceptor score with the ALT allele at the site ≥ 2.

            variants["misplicing_gain"] = (~variants["canonical_splice_site"] & variants["spliceAI_gain_pass"]
                                          & (variants["MaxEntScan_alt"] >= 2))
            #(2) Exon skipping or intron retention (skipping or retention): SpliceAI donor/acceptor loss score at any canonical site ≥ 0.1 without an accompanying gain of mis-splicing by SpliceAI (donor/acceptor gain score < 0.1 at any non-canonical splice site).

            variants["exon_skipping"] = (variants["canonical_splice_site"] & variants["spliceAI_loss_pass"])
            
            variants_misplicing_gain = variants[variants["misplicing_gain"] == 1]
            per_generation_factor = 1.015 * 10 **-7
            variants_misplicing_gain = variants_misplicing_gain[["Gene", "mu"]].compute()
            
            variants_misplicing_gain_rates = pd.DataFrame(variants_misplicing_gain.groupby("Gene")["mu"].sum()).reset_index()

            variants_misplicing_gain_rates["rate"] = variants_misplicing_gain_rates["mu"]* per_generation_factor
            variants_misplicing_gain_rates["exon_skipping_rate"] = 1 - variants_misplicing_gain_rates["rate"]

            variants_merged = variants.merge(variants_misplicing_gain_rates[["Gene", "exon_skipping_rate"]], on = "Gene")

            variants_merged.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)


rule damage_protein_and_others:
    input:
        os.path.join(aso_data_dir, "genes_category/{type}/splice/{chrom}/_metadata")
    output:
        os.path.join(aso_data_dir, "genes_category/{type}/probably_possibly_amenable/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            variants = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            variants = variants.drop(["DS_AG", "DS_AL", "DS_DG", "DS_DL"], axis = 1)

            ## filter for common variants
            variants["common_variant_.1%"] = (variants["AF"] > 0.001)
            variants["common_variant_1%"] = (variants["AF"] > 0.01)
            
            ## filter for benign variants in clinvar
            variants["clinvar_benign"] = (variants["CLNSIG"].str.contains("benign")) | (variants["CLNSIGCONF"].str.contains("benign"))
            variants = variants[~variants["clinvar_benign"]]

            #protein_coding severity
            variants["protein_coding_severe"] = variants["Consequence"].str.contains("(start_lost|stop_gained)")
            variants["protein_coding_severe_missense"] = (variants["REVEL"] > 0.5)

            # get lof severe and moderate
            variants["lof_severe"] = variants["splice_destroying_severe"] | variants["protein_coding_severe"] | variants["protein_coding_severe_missense"]

            variants["lof_moderate"] = variants["splice_destroying_moderate"] & ~variants["lof_severe"]

            ##get probably_amenable variants
            variants["probably_amenable"] = ~variants["lof_severe"] & ~variants["lof_moderate"] & variants["misplicing_gain"]

            ##get possibly_amenable variants
            variants["possibly_amenable_gain"] = variants["lof_moderate"] & variants["misplicing_gain"]
            variants["possibly_amenable_skipping"] = ~variants["lof_severe"] & variants["exon_skipping"]

            variants = variants.repartition(partition_size="3GB")  
            
            variants.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
            

######################################### pergene calculation for AD #######################################################
rule AD_lof_pergene:
    input:
        rate = os.path.join(aso_data_dir, "genes_category/{type}/probably_possibly_amenable/{chrom}/_metadata"),
    output:
        os.path.join(aso_data_dir, "genes_category/{type}/ad_rate/pergene_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            per_generation_factor = 1.015 * 10 **-7
            variants = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            variants = variants[~variants["common_variant_.1%"]]
            
            variants = variants[["Gene", "mu", "probably_amenable", "possibly_amenable_gain", "possibly_amenable_skipping", "exon_skipping_rate"]]
            
            variants = variants[variants["probably_amenable"] | variants["possibly_amenable_gain"] | variants["possibly_amenable_skipping"]].compute()
            
            variants["possibly_amenable_skipping"] = variants["possibly_amenable_skipping"].fillna(0)
            
            variants["probably_amenable"] = variants["probably_amenable"] * variants["mu"] * per_generation_factor
            variants["possibly_amenable_gain"] = variants["possibly_amenable_gain"] * variants["mu"] * per_generation_factor
            variants["possibly_amenable_skipping"] = variants["possibly_amenable_skipping"] * variants["exon_skipping_rate"] * variants["mu"] * per_generation_factor

            variants_bygene = variants[["Gene", "probably_amenable", "possibly_amenable_gain", "possibly_amenable_skipping"]].groupby("Gene").sum()
            
            variants_bygene = variants_bygene.reset_index()
            
            variants_bygene.to_csv(output[0], sep = "\t", index = None)


rule pergene_combine:
    input:
        [os.path.join(aso_data_dir, f"genes_category/{{type}}/pergene_{chrom}.tsv") for chrom in all_chrom_set]
    output:
        os.path.join(aso_data_dir, "genes_category/{type}/pergene_all.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            ddf = dd.read_csv(input, sep = "\t")
            ddf.to_csv(output[0], sep = "\t", single_file = True, index = None)
            
######################################### pergene calculation  #######################################################
            
rule AR_lof_pergene:
    input:
        rate = os.path.join(aso_data_dir, "genes_category/{type}/probably_possibly_amenable/{chrom}/_metadata"),
    output:
        os.path.join(aso_data_dir, "genes_category/{type}/ar_rate/pergene_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-1:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            per_generation_factor = 1.015 * 10 **-7
            variants = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")
            
            ## filter for common variants (less stringent than AD)
            variants = variants[~variants["common_variant_1%"]]


            #get various rates
            ## remove overlap between lof_moderate and exon_skipping
            variants["lof_moderate"] = (variants["lof_moderate"]) & (~variants["possibly_amenable_skipping"])
            
            variants["mu"] = variants["mu"].where(~variants["possibly_amenable_skipping"], variants["mu"] * variants["exon_skipping_rate"])
            variants["mu"] = variants["mu"] * per_generation_factor
            
            variants_lof_severe = variants[variants["lof_severe"]].compute()
            variants_lof_severe = variants_lof_severe[["Gene", "mu", "AF"]].groupby("Gene").sum().reset_index()

            variants_lof_moderate = variants[variants["lof_moderate"]].compute()
            variants_lof_moderate = variants_lof_moderate[["Gene", "mu", "AF"]].groupby("Gene").sum().reset_index()
            
            variants_lof_probably_amenable = variants[variants["probably_amenable"]].compute()
            variants_lof_probably_amenable = variants_lof_probably_amenable[["Gene", "mu", "AF"]].groupby("Gene").sum().reset_index()

            variants_possibly_amenable_skipping = variants[variants["possibly_amenable_skipping"]].compute()
            variants_possibly_amenable_skipping = variants_possibly_amenable_skipping[["Gene", "mu", "AF"]].groupby("Gene").sum().reset_index()
          
            
            variants_all = variants_lof_severe.merge(variants_lof_moderate, on = "Gene", how = "outer", suffixes=('_lof_severe', '_lof_moderate'))
            variants_all = variants_all.merge(variants_lof_probably_amenable, on = "Gene", how = "outer")
            variants_all = variants_all.merge(variants_possibly_amenable_skipping, on = "Gene", how = "outer", suffixes=('_lof_probably_amenable', '_possibly_amenable_skipping'))

            variants_all = variants_all.fillna(0)

            for tail in ["_lof_severe", "_lof_moderate", "_lof_probably_amenable", "_possibly_amenable_skipping"]:
                variants_all[f"total_rate{tail}"] = variants_all[f"mu{tail}"] + variants_all[f"AF{tail}"]

            ## assume 0.5 penetrance
            variants_all["total_lof_rate"] = variants_all["total_rate_lof_severe"] + 0.5 * variants_all["total_rate_lof_moderate"] + 0.5 *variants_all["total_rate_lof_probably_amenable"]+ 0.5 *variants_all["total_rate_possibly_amenable_skipping"]

            variants_all["comp_het_probably_amenable"] = variants_all["total_lof_rate"]**2 - (variants_all["total_rate_lof_severe"] + 0.5* variants_all["total_rate_lof_moderate"] +0.5*  variants_all["total_rate_possibly_amenable_skipping"])**2

            variants_all["comp_het_possibly_amenable_skipping"] = variants_all["total_lof_rate"]**2 - (variants_all["total_rate_lof_severe"] +0.5*  variants_all["total_rate_lof_moderate"] + 0.5* variants_all["total_rate_lof_probably_amenable"])**2
            
            
            variants_all.to_csv(output[0], sep = "\t", index = None)

            
            