import os
import sys
import glob
import numpy as np
import pandas as pd
import math
import sys
import random
import pickle
import dask.dataframe as dd
from dask.distributed import Client, LocalCluster

import csv

sys.path.insert(0,'/home/djl34/lab_pd/bin')
import genomic

pd_data_dir = "/home/djl34/lab_pd/data"
KL_data_dir = "/home/djl34/lab_pd/kl/data"
aso_data_dir = "/home/djl34/lab_pd/aso/data"
scratch_dir = "/n/scratch3/users/d/djl34"

vep = "/home/djl34/bin/ensembl-vep/vep"

base_set = ["A", "C", "T", "G"]
chrom_set = [str(x) for x in range(1, 23)]
# chrom_set = ["22"]
# chrom_set = 

factor = 1

def get_mem_mb(wildcards, attempt):
    return attempt * 20000 * factor

def get_mem_mb_small(wildcards, attempt):
    return attempt * 7000

wildcard_constraints:
    chrom="\d+"
    

    
##############################################################################################################################

## for maxentscan        
# output_list = [os.path.join(scratch_dir, "downloads/whole_gene/" + header + "_rate_v5.2_TFBS_correction_wholegene.MaxEntScan.vcf") for header in header_list]

# header_list = glob.glob(os.path.join(scratch_dir, "downloads/whole_gene/*_regions_split_*.tsv"))

# header_list = [x.split(".")[0] for x in header_list]

# output_list = [x + "_rate_v5.2_TFBS_correction_wholegene.MaxEntScan.txt" for x in header_list]



rule all:
    input:
        [os.path.join(aso_data_dir, "whole_gene/maxentscan/" + chrom + "/_metadata") for chrom in chrom_set],
        [os.path.join(aso_data_dir, "whole_gene/mu/" + chrom + "/_metadata") for chrom in chrom_set],
#         [os.path.join(aso_data_dir, "spliceai/spliceai_delta_scores.raw.snv.chrom_" + chrom + ".tsv") for chrom in chrom_set],
        [os.path.join(aso_data_dir, "whole_gene/spliceai_vcf/" + chrom + "/_metadata") for chrom in chrom_set],
        [os.path.join(aso_data_dir, "whole_gene/LaBranchoR/" + chrom + "/_metadata") for chrom in chrom_set],
        [os.path.join(aso_data_dir, "whole_gene/ad/" + chrom + ".tsv") for chrom in chrom_set],
        [os.path.join(aso_data_dir, "whole_gene/ad/splicing_" + chrom + ".tsv") for chrom in chrom_set],

###################################################### get gnomAD #######################################################
rule download_gnomad_v2:
    input:
    output:
        os.path.join(scratch_dir, "downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{chrom}.vcf.bgz")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    shell:
        "wget -P /n/scratch3/users/d/djl34/downloads/gnomAD_v2/ https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/vcf/exomes/gnomad.exomes.r2.1.1.sites.{wildcards.chrom}.vcf.bgz"
        
rule download_gnomad_v2_tbi:
    input:
    output:
        os.path.join(scratch_dir, "downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{chrom}.vcf.bgz.tbi")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    shell:
        "wget -P /n/scratch3/users/d/djl34/downloads/gnomAD_v2/ https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/vcf/exomes/gnomad.exomes.r2.1.1.sites.{wildcards.chrom}.vcf.bgz.tbi"
        
rule run_vep_maxentscan:
    input:
        os.path.join(scratch_dir, "downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{chrom}.vcf.bgz")
    output:
        os.path.join(scratch_dir, "downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{chrom}.MaxEntScan.txt")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    shell:
        """
        module load gcc/6.2.0
        module load perl/5.30.0
        eval `perl -Mlocal::lib=~/perl5-O2`
        {vep} --cache -i /home/djl34/scratch/downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{wildcards.chrom}.vcf.bgz -o /home/djl34/scratch/downloads/gnomAD_v2/gnomad.exomes.r2.1.1.sites.{wildcards.chrom}.MaxEntScan.txt --fork 5 --plugin MaxEntScan,$HOME/.vep/Plugins/fordownload --force_overwrite --no_stats
        """
         
rule gnomAD_v3_to_tsv:
    input:
        os.path.join(scratch_dir, "downloads/gnomad.genomes.v3.1.2.sites.chr{chrom}.vcf.bgz")
    output:
        os.path.join(scratch_dir, "downloads/gnomad.genomes.v3.1.2.sites.chr{chrom}.tsv")
    shell:        
        "bcftools query -f '%CHROM\t%POS\t%REF\t%ALT\t%FILTER\t%AC\t%AN\t%AF\n' {input} > {output}"
        
###################################################### set up Ensembl #######################################################

rule download_ensembl_cache:
    input:
    output:
        "/home/djl34/.vep/homo_sapiens_vep_110_GRCh38.tar.gz"
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    shell:
        """
        open_perl5
        cd $HOME/.vep
        curl -O https://ftp.ensembl.org/pub/release-110/variation/indexed_vep_cache/homo_sapiens_vep_110_GRCh38.tar.gz
        """
        
        
###################################################### for MaxEntScan #######################################################

rule make_regions_file:
    input:
        os.path.join(pd_data_dir, "biomart/ENSG_start_end_110.tsv"),
    output:
        os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_0.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=2000
    run:        
        df = pd.read_csv(input[0], sep = "\t")
        df = df[["Chromosome/scaffold name", "Gene start (bp)", "Gene end (bp)"]]
        df = df[df["Chromosome/scaffold name"] == wildcards.chrom]
        
        df = df.sort_values("Gene start (bp)")
        
        iterator = 0
        
        csvfile = open(os.path.join(scratch_dir, "downloads/whole_gene/" + str(wildcards.chrom) + "_regions_split_"+ str(iterator)+".tsv"), 'w', newline='')
        csvwriter = csv.writer(csvfile, delimiter='\t')
        
        total_length = 0
        
        for index, row in df.iterrows():
            
            chrom = row["Chromosome/scaffold name"]
            start = row["Gene start (bp)"]
            end = row["Gene end (bp)"]
            
            length = end - start
            
            while total_length + length > 1000000:
                add = 1000000 - total_length
                
                new_end = start + add
                
                csvwriter.writerow([chrom, start, new_end])
                
                #move to new file
                iterator += 1
                csvfile = open(os.path.join(scratch_dir, "downloads/whole_gene/" + str(wildcards.chrom) + "_regions_split_" + str(iterator)+ ".tsv"), 'w', newline='')
                csvwriter = csv.writer(csvfile, delimiter='\t')
                total_length = 0
                
                start = new_end + 1
                length = end - start
                
            csvwriter.writerow([chrom, start, end])
            
            total_length += length
            
#             if total_length > 1000000:
#                 total_length = 0
#                 iterator += 1
                
#                 csvfile = open(os.path.join(scratch_dir, "downloads/whole_gene/" + str(wildcards.chrom) + "_regions_split_" + str(iterator)+ ".tsv"), 'w', newline='')
#                 csvwriter = csv.writer(csvfile, delimiter='\t')
                        

rule download_vova_model:
    input:
#         os.path.join(pd_data_dir, "vova_model/{chrom}_rate_v5.2_TFBS_correction.gz")
    output:
        os.path.join(scratch_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz"),
        os.path.join(scratch_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz.csi"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=25000
    run:
        output_filename_0 = output[0].split("/")[-1]
        output_filename_1 = output[1].split("/")[-1]
        
        shell("wget -P /home/djl34/scratch/downloads/ http://genetics.bwh.harvard.edu/downloads/Vova/Roulette/{output_filename_0}")
        shell("wget -P /home/djl34/scratch/downloads/ http://genetics.bwh.harvard.edu/downloads/Vova/Roulette/{output_filename_1}")
        
rule filter_out_intergenic_sites:
    input:
        vcf = os.path.join(scratch_dir, "downloads/{chrom}_rate_v5.2_TFBS_correction_all.vcf.bgz"),
        regions_file = os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_{split}.tsv")
    output:
        os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_{split}_rate_v5.2_TFBS_correction_wholegene.vcf"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=3000
    shell:        
        "bcftools view -R {input.regions_file} -o {output} {input.vcf}"
        
rule run_vep_maxentscan_whole_gene:
    input:
        os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_{split}_rate_v5.2_TFBS_correction_wholegene.vcf"),
    output:
        os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_{split}_rate_v5.2_TFBS_correction_wholegene.MaxEntScan.vcf"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=8,
        mem_mb=32000
    shell:
        """
        module load gcc/6.2.0
        module load perl/5.30.0
        eval `perl -Mlocal::lib=~/perl5-O2`
        {vep} --cache --offline -i {input} -o {output} --fork 8 --vcf --canonical --plugin MaxEntScan,$HOME/.vep/Plugins/fordownload --force_overwrite --no_stats --buffer_size 10000 --fasta /home/djl34/lab_pd/data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna
        """
        
rule run_split_vep:
    input:
        os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_{split}_rate_v5.2_TFBS_correction_wholegene.MaxEntScan.vcf"),
    output:
        os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_{split}_rate_v5.2_TFBS_correction_wholegene.MaxEntScan.txt"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=2000
    shell:
        """
        bcftools +split-vep {input} -f '%CHROM\t%POS\t%REF\t%ALT\t%Consequence\t%Gene\t%Feature\t%MaxEntScan_alt\t%MaxEntScan_diff\t%MaxEntScan_ref\n' -s primary > {output}
        """
        
rule make_parquet:
    input:
        os.path.join(scratch_dir, "downloads/whole_gene/{chrom}_regions_split_0_rate_v5.2_TFBS_correction_wholegene.MaxEntScan.txt"),
    output:
        os.path.join(aso_data_dir, "whole_gene/maxentscan/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            names_list = ['CHROM', 'POS', 'REF', 'ALT', 'Consequence', 'Gene', 'Feature', 'MaxEntScan_alt', 'MaxEntScan_diff', 'MaxEntScan_ref']
            ddf = dd.read_csv(os.path.join(scratch_dir, "downloads/whole_gene/"+ wildcards.chrom + "_regions_split_*_rate_v5.2_TFBS_correction_wholegene.MaxEntScan.txt"), sep = "\t", names = names_list, dtype={'MaxEntScan_alt': 'object','MaxEntScan_diff': 'object', 'MaxEntScan_ref': 'object'})
            
            ddf= ddf.rename(columns={"POS": "Pos", "REF": "Allele_ref", "ALT": "Allele"})
            
            ddf = ddf.repartition(partition_size="3GB")            
            
            ddf.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

###################################################### for mu #######################################################
            
rule add_mu:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/maxentscan/{chrom}/_metadata"),
        mu = os.path.join(KL_data_dir, "whole_genome/mu_filtered/{chrom}/_metadata")
    output:
        os.path.join(aso_data_dir, "whole_gene/mu/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:

            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")

            mu = dd.read_parquet("/".join(input.mu.split("/")[:-1]) + "/")

            mu = mu[["Pos", "Allele", "mu"]]

            rate = rate.merge(mu, on = ["Pos", "Allele"], how = "left")

            rate = rate.repartition(partition_size="3GB")            

            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)   
        
###################################################### for spliceAI #######################################################

rule process_spliceAI_vcf:    
    input:
        vcf = os.path.join(scratch_dir, "downloads/spliceai/spliceai_scores.raw.snv.chrom_{chrom}.hg38.vcf"),
    output:
        os.path.join(aso_data_dir, "spliceai/spliceai_delta_scores.raw.snv.chrom_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            names = ["CHROM", "Pos", "ID","Allele_ref", "Allele", "QUAL", "FILTER", "Spliceai_info"]
            ddf = dd.read_csv(input.vcf, sep = "\t", comment = "#", names = names, dtype={'CHROM': 'int', 'Pos': 'int'})
            
            ddf["Pos"] = dd.to_numeric(ddf['Pos'], errors='coerce').fillna(0).astype(int)
            ddf = ddf[ddf["Pos"].isna() == False]
            ddf = ddf[ddf["Allele_ref"].isna() == False]
            ddf = ddf[ddf["Allele"].isna() == False]
            
            ddf[["Pos", "Allele_ref", "Allele", "Spliceai_info"]].to_csv(output[0], sep = "\t", index = None, single_file = True)


rule add_spliceAI_vcf:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/mu/{chrom}/_metadata"),
        spliceai = os.path.join(aso_data_dir, "spliceai/spliceai_delta_scores.raw.snv.chrom_{chrom}.tsv")
    output:
        os.path.join(aso_data_dir, "whole_gene/spliceai_vcf/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            rate["Pos"] = dd.to_numeric(rate['Pos'], errors='coerce').fillna(0).astype(int)
            
#             names = ["CHROM", "Pos", "ID","Allele_ref", "Allele", "QUAL", "FILTER", "Spliceai_info"]
            ddf = dd.read_csv(input.spliceai, sep = "\t", comment = "#", dtype={'Pos': 'int'})
            rate = rate.merge(ddf[["Pos", "Allele", "Spliceai_info"]], on = ["Pos", "Allele"], how = "left")
            
            rate = rate.repartition(partition_size="3GB")            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

rule add_spliceAI:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/LaBranchoR/{chrom}/_metadata"),
        spliceai = os.path.join(KL_data_dir, "spliceai/spliceai_delta_scores.raw.snv.chrom_{chrom}/_metadata")
    output:
        os.path.join(aso_data_dir, "whole_gene/spliceai/{chrom}/_metadata"),
        os.path.join(aso_data_dir, "whole_gene/spliceai/intermediary_{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=8,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            rate = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            spliceai_files = glob.glob("/".join(input.spliceai.split("/")[:-1]) + "/*.parquet")
            
            first = True
            
            for filename in spliceai_files:
                print(filename, flush = True)
                
                if not first:
                    rate = dd.read_parquet("/".join(output[1].split("/")[:-1]) + "/")
#                     rate.rename(columns={'DS': 'DS_x', 'spliceai_gene': 'spliceai_gene_x', 'DS_AL': 'DS_AL_x', 'DS_DL': 'DS_DL_x', 'DS_AG': 'DS_AG_x', 'DS_DG': 'DS_DG_x'})  
                
                sa = pd.read_parquet(filename)
                rate = rate.merge(sa, on = ["Pos", "Allele"], how = "left", suffixes = ("", "_x"))
                
                
                if not first:
                    column_names = ['DS', 'DS_AL', 'DS_DL', 'DS_AG', 'DS_DG']
                    for column in column_names:
                        rate[column] = rate[column].fillna(0)
                        rate[column + "_x"] = rate[column + "_x"].fillna(0)

                        rate[column] = rate[column] + rate[column + "_x"]

                        rate = rate.drop(column + "_x", axis=1)  


                    rate["spliceai_gene"] = rate["spliceai_gene"].fillna("")
                    rate["spliceai_gene_x"] = rate["spliceai_gene_x"].fillna("")
                    rate["spliceai_gene"] = rate["spliceai_gene"] + rate["spliceai_gene_x"]

                    rate = rate.drop("spliceai_gene_x", axis=1)  

                rate.to_parquet("/".join(output[1].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)
                first = False
                
            rate = rate.repartition(partition_size="3GB")            
            rate.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True)

###################################################### for LaBranchoR #######################################################

rule add_LaBranchoR:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/spliceai_vcf/{chrom}/_metadata"),
        labranchor = aso_data_dir + "/LaBranchoR/lstm.gencode_v19.hg19.top.bed",
    output:
        os.path.join(aso_data_dir, "whole_gene/LaBranchoR/{chrom}/_metadata"),
    resources:
        partition="short",
        runtime="0-5:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            ddf = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            
            names_list = ["Chrom", "start", "end", "LaBranchoR_start", "LaBranchoR_score", "strand"]
            df = pd.read_csv(aso_data_dir + "/LaBranchoR/lstm.gencode_v19.hg19.top.bed", sep = "\t", names = names_list)
            df = df[df["Chrom"] == "chr" + wildcards.chrom]
            
            df["hg38_start"] = df.apply(lambda row: genomic.get_hg38_pos(row["Chrom"], row["start"]), axis=1)
            df["hg38_end"] = df.apply(lambda row: genomic.get_hg38_pos(row["Chrom"], row["end"]), axis=1)
            
            #b/c it is 0 based
            def get_range(start, end):
                return range(start + 1, end + 1)
            
            df = df[df["hg38_start"].isna() == False]
            df = df[df["hg38_end"].isna() == False]
            
            df["hg38_start"] = df["hg38_start"].astype(int)
            df["hg38_end"] = df["hg38_end"].astype(int)
            
            df['hg38_min'] = df[['hg38_start','hg38_end']].min(axis=1)
            df['hg38_max'] = df[['hg38_start','hg38_end']].max(axis=1)

            
            df["Pos"] = df.apply(lambda row: get_range(row["hg38_min"],row["hg38_max"]), axis=1)
            df = df.explode('Pos')
            df["Pos"] = df["Pos"].astype(int)
            
            df["LaBranchoR_distance"] = 0
            
            for distance in [-3, -2, -1, 1, 2, 3]:
                df_nearby = df.copy()
                
                df_nearby["LaBranchoR_distance"] = distance
                df_nearby["Pos"] = df_nearby["Pos"] + distance
                
                df = pd.concat([df, df_nearby])
                
            
            ddf = ddf.merge(df[["Pos", "LaBranchoR_score", "LaBranchoR_distance"]], on = "Pos", how = "left")
            
            ddf["Pos"] = dd.to_numeric(ddf['Pos'], errors='coerce').fillna(0).astype(int)
#             ddf["LaBranchoR_start"] = dd.to_numeric(ddf['LaBranchoR_start'], errors='coerce').fillna(0).astype(float)
#             ddf["LaBranchoR_score"] = dd.to_numeric(ddf['LaBranchoR_score'], errors='coerce').fillna(0).astype(float)

            ddf.to_parquet("/".join(output[0].split("/")[:-1]), write_index = False, compression = "gzip", write_metadata_file = True) 
            

######################################################filter genes #######################################################
rule AD_lof:
    input:
        rate = os.path.join(aso_data_dir, "whole_gene/LaBranchoR/{chrom}/_metadata"),
        disease = aso_data_dir + "/Clingen-Gene-Disease-Summary-2023-06-24.csv",
        dosage = aso_data_dir + "/Clingen-Dosage-Sensitivity-2023-06-25.csv",
        ensg = pd_data_dir + "/biomart/ENSG_Genename_syn.tsv"
    output:
        os.path.join(aso_data_dir, "whole_gene/ad/{chrom}.tsv"),
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=5,
        mem_mb=get_mem_mb
    run:
        with Client() as client:
            
            #get list of genes that are ad and lof
            df = pd.read_csv(input.disease)

            df_ds = pd.read_csv(input.dosage)
            df_ds_haploinsufficient = df_ds[df_ds["HAPLOINSUFFICIENCY"].isin(['Sufficient Evidence for Haploinsufficiency', 'Emerging Evidence for Haploinsufficiency'])]
            
            df_AD = df[df["MOI"] == "AD"]
            
            df_AD_strong = df_AD[df_AD["CLASSIFICATION"].isin(["Strong", "Definitive"])]

            df_AD_strong_lof = df_AD_strong.merge(df_ds_haploinsufficient, on = "GENE SYMBOL", how = "inner")
            
            gene_list = list(df_AD_strong_lof["GENE SYMBOL"].unique())
            
            #add ensg name
            df_ensg = pd.read_csv(input.ensg, sep = "\t")
            df_ensg = df_ensg.rename({"Gene name": "GENE SYMBOL", "Gene stable ID" : "Gene"}, axis = 1)
            df_ensg = df_ensg[["GENE SYMBOL", "Gene", "Chromosome/scaffold name"]].drop_duplicates()
            df_ensg = df_ensg[df_ensg["Chromosome/scaffold name"].str.contains("CHR") == False]
            df_ensg = df_ensg.rename({"Gene name": "GENE SYMBOL", "Gene stable ID" : "Gene"}, axis = 1)

            df_AD_strong_lof_ensg = df_AD_strong_lof.merge(df_ensg, on = "GENE SYMBOL", how = "left")

            ensg_gene_list = list(df_AD_strong_lof_ensg["Gene"].unique())
            
            #read variants file
            variants = dd.read_parquet("/".join(input.rate.split("/")[:-1]) + "/")
            variants_ad = variants[variants["Gene"].isin(ensg_gene_list)]
            
            variants_ad["MaxEntScan_alt"] = variants_ad["MaxEntScan_alt"].replace(".", None)
            variants_ad["MaxEntScan_ref"] = variants_ad["MaxEntScan_ref"].replace(".", None)
            

            #split spliceAI_info
            columns = ["ALLELE", "SYMBOL", "DS_AG", "DS_AL", "DS_DG", "DS_DL", "DP_AG", "DP_AL", "DP_DG", "DP_DL"]

            variants_ad[columns] = variants_ad["Spliceai_info"].str.split("|", expand = True, n = 9)
            
            variants_ad = variants_ad.drop(["ALLELE", "Spliceai_info"], axis = 1)
            
            variants_ad["DS_AG"] = variants_ad["DS_AG"].astype(float)
            variants_ad["DS_AL"] = variants_ad["DS_AL"].astype(float)
            variants_ad["DS_DG"] = variants_ad["DS_DG"].astype(float)
            variants_ad["DS_DL"] = variants_ad["DS_DL"].astype(float)
            
            variants_ad["DS"] = variants_ad[["DS_AG", "DS_AL", "DS_DG", "DS_DL"]].max(axis=1)
            
            variants_ad.to_csv(output[0], single_file = True, sep = "\t", index = None)

######################################################filter genes #######################################################       
rule AD_lof_add_splicing_info:
    input:
        os.path.join(aso_data_dir, "whole_gene/ad/{chrom}.tsv"),
    output:
        os.path.join(aso_data_dir, "whole_gene/ad/splicing_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=1,
        mem_mb=get_mem_mb_small
    run:
        variants = pd.read_csv(input[0], sep = "\t")
        
        cols = ["MaxEntScan_alt", "MaxEntScan_ref"]
        
        variants[cols] = variants[cols].apply(pd.to_numeric,errors='coerce')
        
        ## check whether it is a splice-destroying-variant
        ##  SpliceAI donor/acceptor loss score ≥ 0.1 at a canonical splice site) AND (MaxEntScan donor/ acceptor score with the ALT allele < MaxEntScan donor/acceptor score with the REF allele) AND [(MaxEntScan donor/acceptor score with the ALT allele < 2) OR (MaxEntScan donor/acceptor score with the ALT allele < 0.3 × MaxEntScan donor/acceptor score with the REF allele)].
        
        variants["splice_destroying_severe"] = 1
        variants["splice_destroying_severe"] = variants["splice_destroying_severe"].where((variants["DS_AL"] > 0.1) | (variants["DS_DL"] > 0.1) , 0)
        variants["splice_destroying_severe"] = variants["splice_destroying_severe"].where(variants["MaxEntScan_alt"] < variants["MaxEntScan_ref"] , 0)
        variants["splice_destroying_severe"] = variants["splice_destroying_severe"].where((variants["MaxEntScan_alt"] < 2) | 
                                            (variants["MaxEntScan_alt"] < 0.3 * variants["MaxEntScan_ref"]) , 0)
        
        ## check whether it is a splice-destroying-variant moderate
        ## i) NOT severe (as defined above), (ii) SpliceAI donor/ acceptor loss score at a canonical splice site ≥ 0.1, AND (iii) [MaxEntScan donor/acceptor score with the ALT allele at the site < MaxEntScan donor/acceptor score with the REF allele at the site, MaxEntScan donor/ acceptor score with the ALT allele at the site ≥ 2, AND MaxEntScan donor/acceptor score with the ALT allele at the site ≥ 0.3 × MaxEntScan donor/acceptor score with the REF allele at the site] OR [The variant is ≤3 nt away from the LaBranchoR-predicted branchpoint OR the distance between the LaBranchoR-predicted branchpoint and the site is changed by >3 nt by the variant].

        variants["splice_destroying_moderate"] = 1
        variants["splice_destroying_moderate"] = variants["splice_destroying_moderate"].where((variants["splice_destroying_moderate"] == 0), 0)
        
        variants["splice_destroying_moderate"] = variants["splice_destroying_moderate"].where((variants["DS_AL"] > 0.1) | (variants["DS_DL"] > 0.1) , 0)
        
        variants["splice_destroying_moderate"] = variants["splice_destroying_moderate"].where(((variants["MaxEntScan_alt"] < variants["MaxEntScan_ref"]) & (variants["MaxEntScan_alt"] > 2) & (variants["MaxEntScan_alt"] > 0.3 * variants["MaxEntScan_ref"])) | (((variants["LaBranchoR_distance"] <= 3) & (variants["LaBranchoR_distance"] >= -3))) , 0)
        
        ##check whether misplicing gain
        ##spliceAI donor/ acceptor gain score at a non-canonical site ≥ 0.1 AND (ii) MaxEntScan donor/acceptor score with the ALT allele at the site ≥ 2.
        
        variants["misplicing_gain"] = 1
        variants["misplicing_gain"] = variants["misplicing_gain"].where((variants["DS_AG"] > 0.1) | (variants["DS_DG"] > 0.1) , 0)
        variants["misplicing_gain"] = variants["misplicing_gain"].where(variants["MaxEntScan_alt"] > 2 , 0)
        
        variants.to_csv(output[0], sep = "\t", index = None)

            
#(2) Exon skipping or intron retention (skipping or retention): SpliceAI donor/acceptor loss score at any canonical site ≥ 0.1 without an accompanying gain of mis-splicing by SpliceAI (donor/acceptor gain score < 0.1 at any non-canonical splice site).
rule ad_lof_exon_skippping:
    input:
        os.path.join(aso_data_dir, "whole_gene/ad/{chrom}/_metadata"),
    output:
        os.path.join(aso_data_dir, "whole_gene/ad_exon_skipping_{chrom}.tsv")
    resources:
        partition="short",
        runtime="0-12:00",
        cpus_per_task=2,
        mem_mb=get_mem_mb_small
    run:       
        with Client() as client:
            variants = dd.read_parquet("/".join(input[0].split("/")[:-1]) + "/")

            exon_skip_intron_ret = variants[(variants["DS_AL"] > 0.1) | (variants["DS_DL"] > 0.1)]
            
            exon_skip_intron_ret.to_csv(output[0], sep = "\t", index = None, single_file = True)

        
            